{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27520411",
   "metadata": {},
   "source": [
    "# Handwritten Digits Recognition — Traditional ML Models\n",
    "\n",
    "**This notebook trains and compares traditional machine learning models** (Logistic Regression, K-Nearest Neighbors, Support Vector Machine, Random Forest) on the MNIST / handwritten digits dataset.\n",
    "\n",
    "**How it works:** The notebook tries to load the dataset from the uploaded zip at `/mnt/data/PRCP-1002-HandwrittenDigits.zip`. If not available, it falls back to `sklearn.datasets.load_digits` so you can run the notebook anywhere.\n",
    "\n",
    "**What you'll find:**\n",
    "\n",
    "- Data loading and EDA (shape, class distribution, sample images)\n",
    "- Preprocessing and flattening / scaling\n",
    "- Training: Logistic Regression, KNN, SVM, RandomForest\n",
    "- Performance comparison (accuracy, precision, recall, f1, confusion matrix)\n",
    "- Short discussion and save best model\n",
    "\n",
    "---\n",
    "\n",
    "*Run all cells in order. This notebook is intended to be runnable in Google Colab or local Jupyter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic imports\n",
    "import os, zipfile, warnings, time\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For fallback dataset\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "print('Imports done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a289d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Attempt to find and load the uploaded dataset zip\n",
    "zip_path = '/mnt/data/PRCP-1002-HandwrittenDigits.zip'\n",
    "data_loaded = False\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    print('Found zip at', zip_path)\n",
    "    z = zipfile.ZipFile(zip_path, 'r')\n",
    "    z.extractall('/mnt/data/PRCP-1002-HandwrittenDigits_extracted')\n",
    "    print('Extracted to /mnt/data/PRCP-1002-HandwrittenDigits_extracted')\n",
    "    extracted = os.listdir('/mnt/data/PRCP-1002-HandwrittenDigits_extracted')\n",
    "    print('Contains:', extracted[:20])\n",
    "    csv_files = [f for f in extracted if f.lower().endswith('.csv')]\n",
    "    npz_files = [f for f in extracted if f.lower().endswith('.npz')]\n",
    "    if csv_files:\n",
    "        csv_path = os.path.join('/mnt/data/PRCP-1002-HandwrittenDigits_extracted', csv_files[0])\n",
    "        print('Loading CSV:', csv_path)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if 'label' in df.columns:\n",
    "            y = df['label'].values\n",
    "            X = df.drop(columns=['label']).values\n",
    "        else:\n",
    "            y = df.iloc[:,0].values\n",
    "            X = df.iloc[:,1:].values\n",
    "        data_loaded = True\n",
    "    elif npz_files:\n",
    "        npz_path = os.path.join('/mnt/data/PRCP-1002-HandwrittenDigits_extracted', npz_files[0])\n",
    "        print('Loading NPZ:', npz_path)\n",
    "        with np.load(npz_path) as data:\n",
    "            print('Keys in npz:', list(data.keys()))\n",
    "            if 'x_train' in data and 'y_train' in data:\n",
    "                X = np.concatenate([data['x_train'], data.get('x_test', np.array([]))], axis=0)\n",
    "                y = np.concatenate([data['y_train'], data.get('y_test', np.array([]))], axis=0)\n",
    "            elif 'images' in data and 'labels' in data:\n",
    "                X = data['images']\n",
    "                y = data['labels']\n",
    "            else:\n",
    "                keys = list(data.keys())\n",
    "                if len(keys) >= 2:\n",
    "                    X = data[keys[0]]\n",
    "                    y = data[keys[1]]\n",
    "                else:\n",
    "                    raise ValueError('Unrecognized .npz format')\n",
    "        data_loaded = True\n",
    "    else:\n",
    "        imgs = []\n",
    "        for root, dirs, files in os.walk('/mnt/data/PRCP-1002-HandwrittenDigits_extracted'):\n",
    "            for f in files:\n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    imgs.append(os.path.join(root,f))\n",
    "        if imgs:\n",
    "            print('Found image files - loading first 2000 images for demo.')\n",
    "            from PIL import Image\n",
    "            samples = min(2000, len(imgs))\n",
    "            X_list, y_list = [], []\n",
    "            for img_path in imgs[:samples]:\n",
    "                label = os.path.basename(os.path.dirname(img_path))\n",
    "                try:\n",
    "                    lab = int(label)\n",
    "                except:\n",
    "                    lab = 0\n",
    "                img = Image.open(img_path).convert('L').resize((28,28))\n",
    "                arr = np.array(img).reshape(-1)\n",
    "                X_list.append(arr)\n",
    "                y_list.append(lab)\n",
    "            X = np.array(X_list)\n",
    "            y = np.array(y_list)\n",
    "            data_loaded = True\n",
    "\n",
    "if not data_loaded:\n",
    "    print('Uploaded dataset not found or not readable. Falling back to sklearn.datasets.load_digits (8x8 images).')\n",
    "    digits = load_digits()\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "\n",
    "X = X.astype('float32')\n",
    "print('X shape:', X.shape, 'y shape:', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic EDA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Number of samples:', X.shape[0])\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print('Class distribution:')\n",
    "for u,c in zip(unique, counts):\n",
    "    print(f'  {int(u)} : {c} samples')\n",
    "\n",
    "def show_samples(X, y, n=10):\n",
    "    plt.figure(figsize=(12,2))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(1, n, i+1)\n",
    "        pix = X[i]\n",
    "        L = int(np.sqrt(pix.size))\n",
    "        plt.imshow(pix.reshape(L, L), cmap='gray')\n",
    "        ax.set_title(str(int(y[i])))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_samples(X, y, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b95653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing: flatten (if needed), scale, train-test split\n",
    "n_samples = X.shape[0]\n",
    "if X.ndim > 2:\n",
    "    X_flat = X.reshape(n_samples, -1)\n",
    "else:\n",
    "    X_flat = X.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_flat)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a few traditional models and record training time & accuracy\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, solver='saga', multi_class='multinomial'),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM': SVC(kernel='rbf', gamma='scale'),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print('\\nTraining', name)\n",
    "    t0 = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    t1 = time.time()\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f'{name} accuracy: {acc:.4f} (train time {t1-t0:.1f}s)')\n",
    "    results.append({'model': name, 'accuracy': acc, 'train_time_s': t1-t0, 'model_obj': model})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e4ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Results table\n",
    "results_df = pd.DataFrame([{'Model': r['model'], 'Accuracy': r['accuracy'], 'TrainTime(s)': r['train_time_s']} for r in results])\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best = results[ np.argmax([r['accuracy'] for r in results]) ]\n",
    "best_model = best['model_obj']\n",
    "print('Best model:', best['model'], 'Accuracy:', best['accuracy'])\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "preds = best_model.predict(X_test)\n",
    "print('\\nClassification report for best model:\\n')\n",
    "print(classification_report(y_test, preds, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "disp.plot(ax=ax, cmap='viridis')\n",
    "plt.title(f'Confusion Matrix - {best[\"model\"]}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the best model and scaler to disk using joblib\n",
    "import joblib\n",
    "out_dir = '/mnt/data/handwritten_model_output'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "joblib.dump(best_model, os.path.join(out_dir, f'best_model_{best[\"model\"]}.joblib'))\n",
    "joblib.dump(scaler, os.path.join(out_dir, 'scaler.joblib'))\n",
    "print('Saved best model and scaler to', out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a6b2d",
   "metadata": {},
   "source": [
    "## (Optional) Quick GridSearch for SVM hyperparameters\n",
    "\n",
    "*Run this cell if you want to try tuning SVM — it may take time depending on dataset size.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A small grid search for SVM (uncomment to run). Kept small by design.\n",
    "# params = {'C':[0.1,1], 'gamma':['scale','auto'], 'kernel':['rbf']}\n",
    "# gs = GridSearchCV(SVC(), params, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "# gs.fit(X_train, y_train)\n",
    "# print('Best params', gs.best_params_, 'Best score', gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7a33e",
   "metadata": {},
   "source": [
    "## Conclusion & Next Steps\n",
    "\n",
    "- Models trained: Logistic Regression, KNN, SVM, RandomForest.\n",
    "- The notebook reports accuracy and shows classification report + confusion matrix for the best model.\n",
    "\n",
    "**Next steps (if you want to improve performance):**\n",
    "1. Use a Convolutional Neural Network (CNN) in TensorFlow/Keras — typically yields highest accuracy on MNIST.\n",
    "2. Data augmentation (slight rotations, shifts) to improve generalization.\n",
    "3. More extensive hyperparameter tuning (GridSearch or RandomizedSearchCV).\n",
    "4. If dataset is the original MNIST (28x28), consider models that use image shape (CNN) instead of flattened vectors.\n",
    "\n",
    "---\n",
    "\n",
    "Upload the dataset zip to `/mnt/data/PRCP-1002-HandwrittenDigits.zip` or ensure your data file is present, then run this notebook."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
